{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273a67dc",
   "metadata": {},
   "source": [
    "# Dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a751a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-sqlserver python-dotenv pypdf langchain-openai langchain langchain-core langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c2fe2",
   "metadata": {},
   "source": [
    "### LangSmith Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdafe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5e2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Env File\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"intro.env\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fad48d",
   "metadata": {},
   "source": [
    "### OAI LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5973b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d0a30",
   "metadata": {},
   "source": [
    "### Embeddings setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a717b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a987f",
   "metadata": {},
   "source": [
    "### VectorStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecf9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "local_vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd83ec",
   "metadata": {},
   "source": [
    "### DB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1655c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection string\n",
    "\n",
    "_CONNECTION_STRING = (\n",
    "    \"Driver={{ODBC Driver 18 for SQL Server}};\"\n",
    "    \"Server=tcp:inft-dev.database.windows.net,1433;\"\n",
    "    \"Database=inft;\"\n",
    "    \"Uid={username};\"\n",
    "    \"Pwd={password};\"\n",
    "    \"Encrypt=yes;\"\n",
    "    \"TrustServerCertificate=no;\"\n",
    "    \"Connection Timeout=30;\"\n",
    ").format(\n",
    "    username=os.getenv(\"SQL_SERVER_USER\"),\n",
    "    password=os.getenv(\"SQL_SERVER_PASSWORD\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3661b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_sqlserver import SQLServer_VectorStore\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "sql_vector_store = SQLServer_VectorStore(\n",
    "    connection_string=_CONNECTION_STRING,\n",
    "    distance_strategy=DistanceStrategy.COSINE,  # optional, if not provided, defaults to COSINE\n",
    "    embedding_function=embeddings,  # you can use different embeddings provided in LangChain\n",
    "    embedding_length=1536,\n",
    "    table_name=\"dbo.Web3\",  # using table with a custom name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba36330",
   "metadata": {},
   "source": [
    "# Progetto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba178d",
   "metadata": {},
   "source": [
    "### 1 - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0668ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b329b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "import pypdf\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DIR_PATH)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2999275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1375\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5956d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 36 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "#Chunking\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # chunk size (characters)\n",
    "    chunk_overlap=100,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98d6514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2ca670d-8032-41c1-ab26-057a65689100\n",
      "a60f62f5-350a-4ab9-9fee-28fbff3bbfeb\n",
      "2768ce01-ddc1-4700-8b2d-c0cf5e8544e3\n"
     ]
    }
   ],
   "source": [
    "ids = local_vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "for i in range(3):\n",
    "    print(ids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed0c1c",
   "metadata": {},
   "source": [
    "### 2 - Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e948a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial State\n",
    "\n",
    "answer_history = []\n",
    "user_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e6c5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40110793",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Use the following pieces of context, User History and Answer History to answer the Question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "User History: {user_history}\n",
    "\n",
    "Answer History: {answer_history}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c1129d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "912d1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    answer_history: List[str]\n",
    "    user_history: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebba4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    # Recupera risultati da entrambi i vector store con score\n",
    "    local_results = local_vector_store.similarity_search_with_score(state[\"question\"])\n",
    "    sql_results = sql_vector_store.similarity_search_with_score(state[\"question\"])\n",
    "    \n",
    "    # Unisci i risultati e deduplica in base al contenuto della pagina\n",
    "    all_results = local_results + sql_results\n",
    "\n",
    "    #deduplication\n",
    "    seen = set()\n",
    "    deduped_results = []\n",
    "    for doc, score in all_results:\n",
    "        # Usa il contenuto come chiave per la deduplica\n",
    "        key = doc.page_content.strip()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped_results.append((doc, score))\n",
    "\n",
    "    #sorting\n",
    "    deduped_results.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Restituisci tutti i risultati deduplicati con score\n",
    "    return {\"context\": [doc for doc, score in deduped_results], \"results_with_score\": deduped_results}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "    prompt_text = prompt.format(\n",
    "        question=state[\"question\"], \n",
    "        context=docs_content,\n",
    "        user_history=state[\"user_history\"],\n",
    "        answer_history=state[\"answer_history\"]\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(prompt_text)\n",
    "\n",
    "    #History save\n",
    "    answer_history = state.get(\"answer_history\", [])\n",
    "    answer_history.append(response.content)\n",
    "\n",
    "    user_history = state.get(\"user_history\", [])\n",
    "    user_history.append(state[\"question\"])\n",
    "\n",
    "    return {\"answer\": response.content, \n",
    "            \"answer_history\": answer_history, \n",
    "            \"user_history\": user_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55edd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff222ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Non è possibile determinare il numero di elementi presenti nella tabella SQL basandosi solo sui dati forniti nel contesto. Non ci sono riferimenti espliciti a una tabella SQL o al numero delle sue righe o colonne. Grazie per asking!\n",
      "\n",
      "Answer: Non ci sono riferimenti nel contesto su specifiche tabelle presenti su SQL Server che puoi interrogare. L'informazione sulle tabelle disponibili non è indicata nei dati forniti. Grazie per asking!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Please insert your message! (digita 'exit' per uscire) \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Invoca il grafo passando le history aggiornate\n",
    "    result = graph.invoke({\n",
    "        \"question\": user_input,\n",
    "        \"answer_history\": answer_history,\n",
    "        \"user_history\": user_history\n",
    "    })\n",
    "\n",
    "    print(f'Answer: {result[\"answer\"]}\\n')\n",
    "\n",
    "    # Aggiorna le history per la prossima iterazione\n",
    "    answer_history = result.get(\"answer_history\", [])\n",
    "    user_history = result.get(\"user_history\", [])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
